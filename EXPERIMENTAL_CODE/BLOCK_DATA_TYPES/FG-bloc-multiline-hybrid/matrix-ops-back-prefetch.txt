/*
 * matrix-ops.C
 *
 *  Created on: 6 juil. 2012
 *      Author: martani
 */

#ifndef MATRIX_OPS_C_
#define MATRIX_OPS_C_

#include <stdlib.h>
#include <malloc.h>

#include "matrix-ops.h"
#include "matrix-utils.h"

using namespace LELA;

#ifndef DEFAULT_BLOC_HEIGHT
#define DEFAULT_BLOC_HEIGHT 0
#error "must define DEFAULT_BLOC_HEIGHT"
#endif

#ifndef DEFAULT_BLOC_WIDTH
#define DEFAULT_BLOC_WIDTH 0
#error "must define DEFAULT_BLOC_WIDTH"
#endif

#ifndef NB_ROWS_PER_MULTILINE
#define NB_ROWS_PER_MULTILINE 0
#error "must define NB_ROWS_PER_MULTILINE"
#endif

#define UNROLL_STEP 32

#define __PREFETCH_WRITE	1
#define __PREFETCH_READ		0
#define __PREFETCH_LOCALITY_NO_LOCALITY	0
#define __PREFETCH_LOCALITY_LOW	1
#define __PREFETCH_LOCALITY_MODERATE	2
#define __PREFETCH_LOCALITY_HIGH	3

/**
 * Given a bloc of dense rows (an array of arrays), Zeros its memory
 */
template <typename Element>
inline void memsetBlocToZero(Element** arr, const uint32 nb_lines, const uint32 line_size)
{
	//cout << "in memset" << endl;
	for(uint32 i=0; i<DEFAULT_BLOC_HEIGHT; ++i)
	{
		memset(arr[i], 0, DEFAULT_BLOC_WIDTH * sizeof(Element));
	}
}

/**
 * Copy a SparseBloc to a bloc of dense rows (an array or arrays)
 */
template<typename Ring, typename Index, typename DoubleFlatElement>
inline void copySparseBlocToDenseBlocArray(const Ring& R,
		const SparseMultilineBloc<typename Ring::Element, Index>& bloc,
		DoubleFlatElement** arr)
{
	for(uint32 i=0; i<DEFAULT_BLOC_HEIGHT/2; ++i)
	{
			Index idx;
		typename Ring::Element val1, val2;

		if(bloc[i].empty ())
			continue;

		if(bloc[i].is_sparse ())
			for(uint32 j=0; j<bloc[i].size (); ++j)
			{
				idx = bloc[i].IndexData[j];
				val1 = bloc[i].at_unchecked(0, j);
				val2 = bloc[i].at_unchecked(1, j);

				arr[i*2][idx] = val1;
				arr[i*2+1][idx] = val2;
			}
		else
			for(uint32 j=0; j<DEFAULT_BLOC_WIDTH; ++j)
			{
				val1 = bloc[i].at_unchecked(0, j);
				val2 = bloc[i].at_unchecked(1, j);

				arr[i*2][j] = val1;
				arr[i*2+1][j] = val2;
			}
	}
}

template<typename Ring, typename DoubleFlatElement, typename Index>
inline void copyDenseBlocArrayToSparseBloc(const Ring& R,
		DoubleFlatElement** arr,
		SparseMultilineBloc<typename Ring::Element, Index>& bloc,
		bool reduce_in_Ring = true)
{
	typename Ring::Element e1, e2;
	MultiLineVector<typename Ring::Element, Index> tmp;

	if(reduce_in_Ring)
	{
		for (uint32 i = 0; i < DEFAULT_BLOC_HEIGHT / 2; ++i)
		{
			bloc[i].clear ();
			tmp.clear ();

			for (uint32 j = 0; j < DEFAULT_BLOC_WIDTH; ++j)
			{
				ModularTraits<typename Ring::Element>::reduce(e1, arr[i * 2][j], R._modulus);
				ModularTraits<typename Ring::Element>::reduce(e2, arr[i * 2 + 1][j], R._modulus);

				if (!R.isZero(e1) || !R.isZero(e2))
				{
					tmp.IndexData.push_back(j);
					tmp.ValuesData.push_back(e1);
					tmp.ValuesData.push_back(e2);
				}
			}

			if ((float)tmp.size() / (float)DEFAULT_BLOC_WIDTH < bloc.get_HYBRID_REPRESENTATION_THRESHOLD())
				bloc[i].swap(tmp);
			else
			{
				Index idx = 0;
				for (uint32 j = 0; j < DEFAULT_BLOC_WIDTH; ++j)
					if (idx < tmp.size() && tmp.IndexData[idx] == j)
					{
						bloc[i].ValuesData.push_back(tmp.at_unchecked(0, idx));
						bloc[i].ValuesData.push_back(tmp.at_unchecked(1, idx));
						idx++;
					}
					else
					{
						bloc[i].ValuesData.push_back(0);
						bloc[i].ValuesData.push_back(0);
					}
			}
		}
	}
	else
	{
		for (uint32 i = 0; i < DEFAULT_BLOC_HEIGHT / 2; ++i)
		{
			bloc[i].clear ();
			tmp.clear ();

			for (uint32 j = 0; j < DEFAULT_BLOC_WIDTH; ++j)
			{
				ModularTraits<typename Ring::Element>::reduce(e1, arr[i * 2][j], R._modulus);
				ModularTraits<typename Ring::Element>::reduce(e2, arr[i * 2 + 1][j], R._modulus);

				if (arr[i * 2][j] != 0 || arr[i * 2+1][j] != 0)
				{
					tmp.IndexData.push_back(j);
					tmp.ValuesData.push_back((typename Ring::Element)arr[i * 2][j]);
					tmp.ValuesData.push_back((typename Ring::Element)arr[i * 2+1][j]);
				}
			}

			if ((float)tmp.size() / (float)DEFAULT_BLOC_WIDTH < bloc.get_HYBRID_REPRESENTATION_THRESHOLD())
				bloc[i].swap(tmp);
			else
			{
				Index idx = 0;
				for (uint32 j = 0; j < DEFAULT_BLOC_WIDTH; ++j)
					if (idx < tmp.size() && tmp.IndexData[idx] == j)
					{
						bloc[i].ValuesData.push_back(tmp.at_unchecked(0, idx));
						bloc[i].ValuesData.push_back(tmp.at_unchecked(1, idx));
						idx++;
					}
					else
					{
						bloc[i].ValuesData.push_back(0);
						bloc[i].ValuesData.push_back(0);
					}
			}
		}
	}
}

template <typename Ring>
inline void reduceDenseArrayModulo(const Ring& R, uint64* arr)
{
	typename Ring::Element e;

	for(uint32 j=0; j<DEFAULT_BLOC_WIDTH; ++j)
	{
		ModularTraits<typename Ring::Element>::reduce (e, arr[j], R._modulus);
		arr[j] = (uint64)e;
	}
}

inline void DenseScalMulSub__one_row__array_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const uint64 *arr_source,
		uint64 *arr1,
		uint64 *arr2) __attribute__((always_inline));

inline void DenseScalMulSub__one_row__array_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const uint64 *arr_source,
		uint64 *arr1,
		uint64 *arr2)
{
	if(av1_col1 == 0)
		for(uint32 i=0; i<DEFAULT_BLOC_WIDTH; i+=32)
		{
			//arr2[i] += av2_col1 * arr_source[i];
			__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
			__builtin_prefetch(arr_source+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

			#pragma loop unroll
			for(uint16 t=0; t<32; t+=8)
			{
				__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
				__builtin_prefetch(arr_source +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

				for(uint8 k=0;k<8;++k)
					arr2[i+t+k] += arr_source[i+t+k] * av2_col1;
			}
		}
	else if (av2_col1 == 0)
		for(uint32 i=0; i<DEFAULT_BLOC_WIDTH; i+=32)
		{
			//arr1[i] += av1_col1 * arr_source[i];
			__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
			__builtin_prefetch(arr_source+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

			#pragma loop unroll
			for(uint16 t=0; t<32; t+=8)
			{
				__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
				__builtin_prefetch(arr_source +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

				for(uint8 k=0;k<8;++k)
					arr1[i+t+k] += arr_source[i+t+k] * av1_col1;
			}
		}
	else
		for(uint32 i=0; i<DEFAULT_BLOC_WIDTH; i+=32)
		{
			//arr1[i] += av1_col1 * arr_source[i];
			//arr2[i] += av2_col1 * arr_source[i];
			__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
			__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
			__builtin_prefetch(arr_source+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

			#pragma loop unroll
			for(uint16 t=0; t<32; t+=8)
			{
				__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
				__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
				__builtin_prefetch(arr_source +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

				for(uint8 k=0;k<8;++k)
				{
					arr1[i+t+k] += av1_col1 * arr_source[i+t+k];
					arr2[i+t+k] += av2_col1 * arr_source[i+t+k];
				}
			}
		}
}

inline void DenseScalMulSub__two_rows__array_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const register uint16 av1_col2,
		const register uint16 av2_col2,
		const uint64 *arr_source1,
		const uint64 *arr_source2,
		uint64 *arr1,
		uint64 *arr2) __attribute__((always_inline));

inline void DenseScalMulSub__two_rows__array_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const register uint16 av1_col2,
		const register uint16 av2_col2,
		const uint64 *arr_source1,
		const uint64 *arr_source2,
		uint64 *arr1,
		uint64 *arr2)
{
	if(av1_col1 == 0 && av2_col1 == 0)
	{
		DenseScalMulSub__one_row__array_array(
				av1_col2,
				av2_col2,
				arr_source2,
				arr1,
				arr2);

		return;
	}

	if(av1_col2 == 0 && av2_col2 == 0)
	{
		DenseScalMulSub__one_row__array_array(
				av1_col1,
				av2_col1,
				arr_source1,
				arr1,
				arr2);

		return;
	}

//	for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; ++i)
//	{
//		arr1[i] += av1_col1 * arr_source1[i] + av1_col2 * arr_source2[i];
//		arr2[i] += av2_col1 * arr_source1[i] + av2_col2 * arr_source2[i];
//	}

	if(av1_col1 == 0)
	{
		if(av1_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr2[i] += av2_col1 * arr_source1[i] + av2_col2 * arr_source2[i];
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

					for(uint8 k=0;k<8;++k)
						arr2[i+t+k] += av2_col1 * arr_source1[i+t+k] + av2_col2 * arr_source2[i+t+k];
				}
			}
		}
		else if(av2_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col2 * arr_source2[i];
				//arr2[i] += av2_col1 * arr_source1[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col2 * arr_source2[i+t+k];
						arr2[i+t+k] += av2_col1 * arr_source1[i+t+k];
					}
				}
			}
		}
		else	//both != 0
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col2 * arr_source2[i];
				//arr2[i] += av2_col1 * arr_source1[i] + av2_col2 * arr_source2[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col2 * arr_source2[i+t+k];
						arr2[i+t+k] += av2_col1 * arr_source1[i+t+k] + av2_col2 * arr_source2[i+t+k];
					}
				}
			}
		}
	}
	else if(av2_col1 == 0)
	{
		if(av1_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col1 * arr_source1[i];
				//arr2[i] += av2_col2 * arr_source2[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col1 * arr_source1[i+t+k];
						arr2[i+t+k] += av2_col2 * arr_source2[i+t+k];
					}
				}
			}
		}
		else if(av2_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col1 * arr_source1[i] + av1_col2 * arr_source2[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col1 * arr_source1[i+t+k] + av1_col2 * arr_source2[i+t+k];
					}
				}
			}
		}
		else	//both != 0
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col1 * arr_source1[i] + av1_col2 * arr_source2[i];
				//arr2[i] += av2_col2 * arr_source2[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col1 * arr_source1[i+t+k] + av1_col2 * arr_source2[i+t+k];
						arr2[i+t+k] += av2_col2 * arr_source2[i+t+k];
					}
				}
			}
		}
	}
	else	//both != 0
	{
		if(av1_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col1 * arr_source1[i];
				//arr2[i] += av2_col1 * arr_source1[i] + av2_col2 * arr_source2[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col1 * arr_source1[i+t+k];
						arr2[i+t+k] += av2_col1 * arr_source1[i+t+k] + av2_col2 * arr_source2[i+t+k];
					}
				}
			}
		}
		else if(av2_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col1 * arr_source1[i] + av1_col2 * arr_source2[i];
				//arr2[i] += av2_col1 * arr_source1[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col1 * arr_source1[i+t+k] + av1_col2 * arr_source2[i+t+k];
						arr2[i+t+k] += av2_col1 * arr_source1[i+t+k];
					}
				}
			}
		}
		else	//both != 0
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
				//arr1[i] += av1_col1 * arr_source1[i] + av1_col2 * arr_source2[i];
				//arr2[i] += av2_col1 * arr_source1[i] + av2_col2 * arr_source2[i];
				__builtin_prefetch(arr1 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr2 +i+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);
				__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(arr_source2+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				#pragma loop unroll
				for(uint16 t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2 +i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source2 +i+t+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);		//prefetch every 8*uint64
					__builtin_prefetch(arr_source1+i+8, __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

					for(uint8 k=0;k<8;++k)
					{
						arr1[i+t+k] += av1_col1 * arr_source1[i+t+k] + av1_col2 * arr_source2[i+t+k];
						arr2[i+t+k] += av2_col1 * arr_source1[i+t+k] + av2_col2 * arr_source2[i+t+k];
					}
				}
			}
		}
	}

}


template <typename Index>
inline void SparseScalMulSub__one_row__vect_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const MultiLineVector<uint16, Index>& v,
		const uint16 line,
		uint64 *arr1,
		uint64 *arr2) __attribute__((always_inline));
template <typename Index>
inline void SparseScalMulSub__one_row__vect_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const MultiLineVector<uint16, Index>& v,
		const uint16 line,
		uint64 *arr1,
		uint64 *arr2)
{
//	lela_check(line < v.nb_lines());
	const uint32 sz = v.size();

	/*for(uint32 i=0; i<v.size (); ++i)
	{
		idx = v.IndexData[i];

		val1 = v.at_unchecked(line, i);

		arr1[idx] += (uint32) av1_col1 * val1;
		arr2[idx] += (uint32) av2_col1 * val1;
	}*/


	const uint8 xl = v.size() % UNROLL_STEP;
	register uint32 x = 0;


	if (av1_col1 != 0 && av2_col1 != 0) //cannot both be 0
	{
		register uint32 idx;
		register uint16 val1;
		while (x < xl)
		{
			idx = v.IndexData[x];
			val1 = v.at_unchecked(line, x);
			arr1[idx] += (uint32) av1_col1 * val1;
			arr2[idx] += (uint32) av2_col1 * val1;

			++x;
		}

		__builtin_prefetch(&(v.IndexData._index_vector[x]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
		__builtin_prefetch(&(v.ValuesData._data[x*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);

		for (x = xl; x < sz; x += UNROLL_STEP)
		{
			__builtin_prefetch(&(v.IndexData._index_vector[x]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
			__builtin_prefetch(&(v.ValuesData._data[x*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
//#pragma loop unroll
			for (uint8 t = 0; t < UNROLL_STEP; ++t)
			{
				idx = v.IndexData[x + t];
				val1 = v.at_unchecked(line, x + t);
				arr1[idx] += (uint32) av1_col1 * val1;
				arr2[idx] += (uint32) av2_col1 * val1;
			}
		}
	}
	else if (av1_col1 != 0)
	{
		register uint32 idx;
		register uint16 val1;
		while (x < xl)
		{
			idx = v.IndexData[x];
			val1 = v.at_unchecked(line, x);
			arr1[idx] += (uint32) av1_col1 * val1;

			++x;
		}

		__builtin_prefetch(&(v.IndexData._index_vector[x]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
		__builtin_prefetch(&(v.ValuesData._data[x*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);

		for (x = xl; x < sz; x += UNROLL_STEP)
		{

			__builtin_prefetch(&(v.IndexData._index_vector[x]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
			__builtin_prefetch(&(v.ValuesData._data[x*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
//#pragma loop unroll
			for (uint8 t = 0; t < UNROLL_STEP; ++t)
			{
				idx = v.IndexData[x + t];
				val1 = v.at_unchecked(line, x + t);
				arr1[idx] += (uint32) av1_col1 * val1;
			}
		}
	}
	else //av2_col1 != 0
	{
		register uint32 idx;
		register uint16 val1;
		while (x < xl)
		{
			idx = v.IndexData[x];
			val1 = v.at_unchecked(line, x);
			arr2[idx] += (uint32) av2_col1 * val1;

			++x;
		}

		__builtin_prefetch(&(v.IndexData._index_vector[x]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
		__builtin_prefetch(&(v.ValuesData._data[x*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);

		for (x = xl; x < sz; x += UNROLL_STEP)
		{

			__builtin_prefetch(&(v.IndexData._index_vector[x]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
			__builtin_prefetch(&(v.ValuesData._data[x*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
//#pragma loop unroll
			for (uint8 t = 0; t < UNROLL_STEP; ++t)
			{
				idx = v.IndexData[x + t];
				val1 = v.at_unchecked(line, x + t);
				arr2[idx] += (uint32) av2_col1 * val1;
			}
		}
	}

}


template <typename Index>
inline void DenseScalMulSub__one_row__vect_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const MultiLineVector<uint16, Index>& v,
		const uint16 line,
		uint64 *arr1,
		uint64 *arr2) __attribute__((always_inline));

template <typename Index>
inline void DenseScalMulSub__one_row__vect_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const MultiLineVector<uint16, Index>& v,
		const uint16 line,
		uint64 *arr1,
		uint64 *arr2)
{
//	check_equal_or_raise_exception(v.is_sparse(), false);

	register uint16 val1;

	if(av1_col1 == 0)
		for(uint32 i=0; i<DEFAULT_BLOC_WIDTH; i+=32)
		{
			__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

			//val1 = v.at_unchecked(line, i);
			//arr2[i] += (uint32) av2_col1 * val1;
			for(Index t=0; t<32; t+=8)
			{
				__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

				for(uint8 k=0;k<8;++k)
				{
					val1 = v.at_unchecked(line, i+t+k);
					arr2[i+t+k] += (uint32) av2_col1 * val1;
				}
			}
		}
	else if(av2_col1 == 0)
		for(uint32 i=0; i<DEFAULT_BLOC_WIDTH; i+=32)
		{
			//val1 = v.at_unchecked(line, i);
			//arr1[i] += (uint32) av1_col1 * val1;
			__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

			for(Index t=0; t<32; t+=8)
			{
				__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

				for(uint8 k=0;k<8;++k)
				{
					val1 = v.at_unchecked(line, i+t+k);
					arr1[i+t+k] += (uint32) av1_col1 * val1;
				}
			}
		}
	else
		for(uint32 i=0; i<DEFAULT_BLOC_WIDTH; i+=32)
		{
			//val1 = v.at_unchecked(line, i);
			//arr1[i] += (uint32) av1_col1 * val1;
			//arr2[i] += (uint32) av2_col1 * val1;
			__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

			for(Index t=0; t<32; t+=8)
			{
				__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
				__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

				for(uint8 k=0;k<8;++k)
				{
					val1 = v.at_unchecked(line, i+t+k);
					arr1[i+t+k] += (uint32) av1_col1 * val1;
					arr2[i+t+k] += (uint32) av2_col1 * val1;
				}
			}
		}
}


template <typename Index>
inline void DenseScalMulSub__two_rows__vect_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const register uint16 av1_col2,
		const register uint16 av2_col2,
		const MultiLineVector<uint16, Index>& v,
		uint64 *arr1,
		uint64 *arr2) __attribute__((always_inline));

template <typename Index>
inline void DenseScalMulSub__two_rows__vect_array(const register uint16 av1_col1,
		const register uint16 av2_col1,
		const register uint16 av1_col2,
		const register uint16 av2_col2,
		const MultiLineVector<uint16, Index>& v,
		uint64 *arr1,
		uint64 *arr2)
{
//	check_equal_or_raise_exception(v.is_sparse(), false);

	if(av1_col1 == 0 && av2_col1 == 0)
	{
		DenseScalMulSub__one_row__vect_array(
				av1_col2,
				av2_col2,
				v,
				1,
				arr1,
				arr2);

		return;
	}

	if(av1_col2 == 0 && av2_col2 == 0)
	{
		DenseScalMulSub__one_row__vect_array(
				av1_col1,
				av2_col1,
				v,
				0,
				arr1,
				arr2);

		return;
	}

	register uint16 val1, val2;

//	for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; ++i)
//	{
//		val1 = v.at_unchecked(0, i);
//		val2 = v.at_unchecked(1, i);
//
//		arr1[i] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
//		arr2[i] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
//	}

	if(av1_col1 == 0)
	{
		if(av1_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//				arr2[i] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr2[i+t+k] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
					}
				}
			}
		}
		else if(av2_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//				arr1[i] += (uint64)((uint32) av1_col2 * val2);
//				arr2[i] += (uint64)((uint32) av2_col1 * val1);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint32) av1_col2 * val2;
						arr2[i+t+k] += (uint32) av2_col1 * val1;
					}
				}
			}
		}
		else	//both != 0
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//				arr1[i] += (uint64)((uint32) av1_col2 * val2);
//				arr2[i] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)((uint32) av1_col2 * val2);
						arr2[i+t+k] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
					}
				}
			}
		}
	}
	else if(av2_col1 == 0)
	{
		if(av1_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//				arr1[i] += (uint64)((uint32) av1_col1 * val1);
//				arr2[i] += (uint64)((uint32) av2_col2 * val2);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)((uint32) av1_col1 * val1);
						arr2[i+t+k] += (uint64)((uint32) av2_col2 * val2);
					}
				}
			}
		}
		else if(av2_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//				arr1[i] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
					}
				}

			}
		}
		else	//both != 0
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//				arr1[i] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
//				arr2[i] += (uint64)((uint32) av2_col2 * val2);

				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
						arr2[i+t+k] += (uint64)((uint32) av2_col2 * val2);
					}
				}

			}
		}
	}
	else	//both != 0
	{
		if(av1_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//
//				arr1[i] += (uint64)((uint32) av1_col1 * val1);
//				arr2[i] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));

				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					//__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					//__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)((uint32) av1_col1 * val1);
						arr2[i+t+k] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));
					}
				}
			}
		}
		else if(av2_col2 == 0)
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//
//				arr1[i] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));
//				arr2[i] += (uint64)((uint32) av2_col1 * val1);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					//__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					//__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));
						arr2[i+t+k] += (uint64)((uint32) av2_col1 * val1);
					}
				}
			}
		}
		else	//both != 0
		{
			for (uint32 i = 0; i < DEFAULT_BLOC_WIDTH; i+=32)
			{
//				val1 = v.at_unchecked(0, i);
//				val2 = v.at_unchecked(1, i);
//
//				arr1[i] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
//				arr2[i] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
				__builtin_prefetch(&(v.ValuesData._data[i*NB_ROWS_PER_MULTILINE+64]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);

				for(Index t=0; t<32; t+=8)
				{
					//__builtin_prefetch(arr1+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64
					//__builtin_prefetch(arr2+ i+t+8, __PREFETCH_WRITE, __PREFETCH_LOCALITY_NO_LOCALITY);		//prefetch every 8*uint64

					for(uint8 k=0;k<8;++k)
					{
						val1 = v.at_unchecked(0, i+t+k);
						val2 = v.at_unchecked(1, i+t+k);
						arr1[i+t+k] += (uint64)((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2);
						arr2[i+t+k] += (uint64)((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2);
					}
				}
			}
		}
	}
}


template <typename Index>
inline void SparseScalMulSub__two_rows__vect_array(const uint16 av1_col1,
		const uint16 av2_col1,
		const uint16 av1_col2,
		const uint16 av2_col2,
		const MultiLineVector<uint16, Index>& v,
		uint64 *arr1,
		uint64 *arr2) __attribute__((always_inline));
//AXPY2
template <typename Index>
inline void SparseScalMulSub__two_rows__vect_array(const uint16 av1_col1,
		const uint16 av2_col1,
		const uint16 av1_col2,
		const uint16 av2_col2,
		const MultiLineVector<uint16, Index>& v,
		uint64 *arr1,
		uint64 *arr2)
{
	register uint32 idx;
	register uint16 val1, val2;

	/*for (uint32 i = 0; i < v.size(); ++i)
	{
		idx = v.IndexData[i];

		val1 = v.at_unchecked(0, i);
		val2 = v.at_unchecked(1, i);

		arr1[idx] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));
		arr2[idx] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));
	}

	return;*/
	/*			START_UNROLL_CODE
				arr1[idx] += (uint32) av1_col1 * val1;
				arr1[idx] += (uint32) av1_col2 * val2;

				arr2[idx] += (uint32) av2_col1 * val1;
				arr2[idx] += (uint32) av2_col2 * val2;
				MIDDLE_UNROLL_CODE
#pragma loop unroll
			MIDDLE_UNROLL_CODE2
					arr1[idx] += (uint32) av1_col1 * val1;
					arr1[idx] += (uint32) av1_col2 * val2;

					arr2[idx] += (uint32) av2_col1 * val1;
					arr2[idx] += (uint32) av2_col2 * val2;
				END_UNROLL_CODE
*/
//	if(av1_col1 == 0 && av2_col1 == 0)
//	{
//		axpy(av1_col2, av2_col2, v, 1, arr1, arr2);
//		return;
//	}
//
//	if(av1_col2 == 0 && av2_col2 == 0)
//	{
//		axpy(av1_col1, av2_col1, v, 0, arr1, arr2);
//		return;
//	}

	if (av1_col1 == 0)
	{
		if (av1_col2 == 0)
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				//arr1[idx] += (uint32) av1_col1 * val1;
				//arr1[idx] += (uint32) av1_col2 * val2;

				arr2[idx] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));
			}
		else if (av2_col2 == 0)
		{
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				//arr1[idx] += (uint32) av1_col1 * val1;
				arr1[idx] += (uint32) av1_col2 * val2;

				arr2[idx] += (uint32) av2_col1 * val1;
				//arr2[idx] += (uint32) av2_col2 * val2;
			}
		}
		else
		{
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				//arr1[idx] += (uint32) av1_col1 * val1;
				arr1[idx] += (uint32) av1_col2 * val2;

				arr2[idx] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));
			}
		}
	}
	else if (av2_col1 == 0)
	{
		if (av1_col2 == 0)
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				arr1[idx] += (uint32) av1_col1 * val1;
				//arr1[idx] += (uint32) av1_col2 * val2;

				//arr2[idx] += (uint32) av2_col1 * val1;
				arr2[idx] += (uint32) av2_col2 * val2;
			}
		else if (av2_col2 == 0)
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				arr1[idx] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));

				//arr2[idx] += (uint32) av2_col1 * val1;
				//arr2[idx] += (uint32) av2_col2 * val2;
			}
		else
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				arr1[idx] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));

				//arr2[idx] += (uint32) av2_col1 * val1;
				arr2[idx] += (uint32) av2_col2 * val2;
			}
	}
	else // av1_col1 && av2_col1 != 0
	{
		if (av1_col2 == 0)
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				arr1[idx] += (uint32) av1_col1 * val1;
				//arr1[idx] += (uint32) av1_col2 * val2;

				arr2[idx] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));
			}
		else if (av2_col2 == 0)
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				arr1[idx] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));

				arr2[idx] += (uint32) av2_col1 * val1;
				//arr2[idx] += (uint32) av2_col2 * val2;
			}
		else
			for (uint32 i = 0; i < v.size(); ++i)
			{
				idx = v.IndexData[i];

				val1 = v.at_unchecked(0, i);
				val2 = v.at_unchecked(1, i);

				arr1[idx] += (uint64)(((uint32) av1_col1 * val1) + (uint64)((uint32) av1_col2 * val2));
				arr2[idx] += (uint64)(((uint32) av2_col1 * val1) + (uint64)((uint32) av2_col2 * val2));
			}
	}
}


TIMER_DECLARE_(reduceDenseArrayModulo);
TIMER_DECLARE_(DenseScalMulSub__one_row__array_array);
TIMER_DECLARE_(DenseScalMulSub__two_rows__array_array);
TIMER_DECLARE_(SparseScalMulSub__one_row__vect_array);
TIMER_DECLARE_(DenseScalMulSub__one_row__vect_array);
TIMER_DECLARE_(DenseScalMulSub__two_rows__vect_array);
TIMER_DECLARE_(SparseScalMulSub__two_rows__vect_array);

TIMER_DECLARE_(reduceBlocByRectangularBlocOuter);
TIMER_DECLARE_(reduceBlocByRectangularBlocInner);

template <typename Index>
inline void reduceBlocByRectangularBloc(const Modular<uint16>& R,
		const SparseMultilineBloc<uint16, Index>& bloc_A,
		const SparseMultilineBloc<uint16, Index>& bloc_B,
		uint64 **Bloc_acc) __attribute__((always_inline));

template <typename Index>
inline void reduceBlocByRectangularBloc(const Modular<uint16>& R,
		const SparseMultilineBloc<uint16, Index>& bloc_A,
		const SparseMultilineBloc<uint16, Index>& bloc_B,
		uint64 **Bloc_acc)
{
	typedef Modular<uint16> Ring;

	//	if(bloc_A.empty() || bloc_B.empty())
	//		return;

	TIMER_START_(reduceBlocByRectangularBlocOuter);
	//for all the rows in Bloc_acc (same as for Bloc_A)
	for(uint16 i=0; i<DEFAULT_BLOC_HEIGHT/2; ++i)
	{
		const Index sz = bloc_A[i].size();

		for (uint32 j = 0; j < (int)sz; ++j)
		{
			register typename Ring::Element Av1_col1, Av2_col1;
			const uint32 Ap1 = bloc_A[i].IndexData[j];

			R.copy(Av1_col1, bloc_A[i].at_unchecked(0, j));
			R.copy(Av2_col1, bloc_A[i].at_unchecked(1, j));

//			if (Av1_col1 != 0)
//				R.negin(Av1_col1);
//			if (Av2_col1 != 0)
//				R.negin(Av2_col1);

			R.negin(Av1_col1);
			R.negin(Av2_col1);

			TIMER_START_(reduceBlocByRectangularBlocInner);
			if (((Ap1 & 1) != 1) && (j < (int)sz - 1))
			{
				const uint32 Ap2 = bloc_A[i].IndexData[j + 1];
				if (Ap2 == Ap1 + 1) //axpy 2 ROWS
				{
					register typename Ring::Element Av1_col2, Av2_col2;

					R.copy(Av1_col2, bloc_A[i].at_unchecked(0, j+1));
					R.copy(Av2_col2, bloc_A[i].at_unchecked(1, j+1));

//					if (Av1_col2 != 0)
//						R.negin(Av1_col2);
//					if (Av2_col2 != 0)
//						R.negin(Av2_col2);

					R.negin(Av1_col2);
					R.negin(Av2_col2);

					++j;

					if (bloc_B[Ap1 / NB_ROWS_PER_MULTILINE].is_sparse ())
					{
						TIMER_START_(SparseScalMulSub__two_rows__vect_array);
						SparseScalMulSub__two_rows__vect_array(
								Av1_col1,
								Av2_col1,
								Av1_col2,
								Av2_col2,
								bloc_B[Ap1 / NB_ROWS_PER_MULTILINE],
								Bloc_acc[i * 2],
								Bloc_acc[i * 2 + 1]);
						TIMER_STOP_(SparseScalMulSub__two_rows__vect_array);
					}
					else
					{
						TIMER_START_(DenseScalMulSub__two_rows__vect_array);
						DenseScalMulSub__two_rows__vect_array(
								Av1_col1,
								Av2_col1,
								Av1_col2,
								Av2_col2,
								bloc_B[Ap1 / NB_ROWS_PER_MULTILINE],
								Bloc_acc[i * 2],
								Bloc_acc[i * 2 + 1]);
						TIMER_STOP_(DenseScalMulSub__two_rows__vect_array);
					}
				}
				else	//axpy ONE ROW
				{
					if (bloc_B[Ap1 / NB_ROWS_PER_MULTILINE].is_sparse ())
					{
						TIMER_START_(SparseScalMulSub__one_row__vect_array);
						SparseScalMulSub__one_row__vect_array(
								Av1_col1,
								Av2_col1,
								bloc_B[Ap1 / NB_ROWS_PER_MULTILINE],
								Ap1 % NB_ROWS_PER_MULTILINE,
								Bloc_acc[i * 2],
								Bloc_acc[i * 2 + 1]);
						TIMER_STOP_(SparseScalMulSub__one_row__vect_array);
					}
					else
					{
						TIMER_START_(DenseScalMulSub__one_row__vect_array);
						DenseScalMulSub__one_row__vect_array(
								Av1_col1,
								Av2_col1,
								bloc_B[Ap1 / NB_ROWS_PER_MULTILINE],
								Ap1 % NB_ROWS_PER_MULTILINE,
								Bloc_acc[i * 2],
								Bloc_acc[i * 2 + 1]);
						TIMER_STOP_(DenseScalMulSub__one_row__vect_array);
					}
				}
			}
			else	//axpy ONE ROW
			{
					if (bloc_B[Ap1 / NB_ROWS_PER_MULTILINE].is_sparse ())
					{
						TIMER_START_(SparseScalMulSub__one_row__vect_array);
							SparseScalMulSub__one_row__vect_array(
									Av1_col1,
									Av2_col1,
									bloc_B[Ap1 / NB_ROWS_PER_MULTILINE],
									Ap1 % NB_ROWS_PER_MULTILINE,
									Bloc_acc[i * 2],
									Bloc_acc[i * 2 + 1]);
						TIMER_STOP_(SparseScalMulSub__one_row__vect_array);
					}
					else
					{
						TIMER_START_(DenseScalMulSub__one_row__vect_array);
						DenseScalMulSub__one_row__vect_array(
								Av1_col1,
								Av2_col1,
								bloc_B[Ap1 / NB_ROWS_PER_MULTILINE],
								Ap1 % NB_ROWS_PER_MULTILINE,
								Bloc_acc[i * 2],
								Bloc_acc[i * 2 + 1]);
						TIMER_STOP_(DenseScalMulSub__one_row__vect_array);
					}

			}
			TIMER_STOP_(reduceBlocByRectangularBlocInner);

			//__builtin_prefetch (&(bloc_A[i].IndexData[j+1]), __PREFETCH_READ, __PREFETCH_LOCALITY_LOW);
			//__builtin_prefetch (&(bloc_A[i].ValuesData[(j+1)*NB_ROWS_PER_MULTILINE]), __PREFETCH_READ, __PREFETCH_LOCALITY_MODERATE);
		}
	}
	TIMER_STOP_(reduceBlocByRectangularBlocOuter);

	//cout << "1 LINE Sparse "<< sp1l << " DENSE " << de1l << endl;
	//cout << "2 LINE Sparse "<< sp2l << " DENSE " << de2l << endl;
}


/**
 * Reduce the rows inside the bloc by themselves
 */
template<typename Index>
inline void reduceBlocByTriangularBloc(const Modular<uint16>& R,
		const SparseMultilineBloc<uint16, Index>& bloc_A,
		uint64** Bloc_acc) __attribute__((always_inline));

template<typename Index>
inline void reduceBlocByTriangularBloc(const Modular<uint16>& R,
		const SparseMultilineBloc<uint16, Index>& bloc_A,
		uint64** Bloc_acc)
{
	typedef Modular<uint16> Ring;


	for(uint16 i=0; i<DEFAULT_BLOC_HEIGHT/2; ++i)
	{
		if(bloc_A[i].empty ())
			continue;

		if(bloc_A[i].is_sparse ())
		{
			int last_idx = -1;
			if(bloc_A[i].at_unchecked(1, bloc_A[i].size()-1) == 0)
				last_idx = (int)bloc_A[i].size()-1;
			else
				last_idx = (int)bloc_A[i].size()-2;

			typename Ring::Element Av1_col1, Av2_col1;
			uint32 Ap1;
			typename Ring::Element Av1_col2, Av2_col2;
			uint32 Ap2;

			for (int j = 0; j < last_idx; ++j)	//skip first two elements
			{
				Ap1 = (uint32)bloc_A[i].IndexData[j];
				R.copy(Av1_col1, bloc_A[i].at_unchecked(0, j));
				R.copy(Av2_col1, bloc_A[i].at_unchecked(1, j));

				if (Av1_col1 != 0)
					R.negin(Av1_col1);
				if (Av2_col1 != 0)
					R.negin(Av2_col1);

				if(Ap1 >= i*2)
					throw std::runtime_error ("Index pointing to an out of range line.");

				if (Ap1 % 2 == 0 && j < last_idx - 1)
				{
					Ap2 = bloc_A[i].IndexData[j+1];
					if (Ap2 == Ap1+1)				//axpy TWO ARRAYS
					{
						R.copy(Av1_col2, bloc_A[i].at_unchecked(0, j+1));
						R.copy(Av2_col2, bloc_A[i].at_unchecked(1, j+1));

						if (Av1_col2 != 0)
							R.negin(Av1_col2);
						if (Av2_col2 != 0)
							R.negin(Av2_col2);

						++j;

						TIMER_START_(DenseScalMulSub__two_rows__array_array);
							DenseScalMulSub__two_rows__array_array(
									Av1_col1,
									Av2_col1,
									Av1_col2,
									Av2_col2,
									Bloc_acc[Ap1],
									Bloc_acc[Ap1+1],
									Bloc_acc[i*2],
									Bloc_acc[i*2+1]);
						TIMER_STOP_(DenseScalMulSub__two_rows__array_array);
					}
					else	//axpy ONE ARRAY
					{
						TIMER_START_(DenseScalMulSub__one_row__array_array);
							DenseScalMulSub__one_row__array_array(
									Av1_col1,
									Av2_col1,
									Bloc_acc[Ap1],
									Bloc_acc[i*2],
									Bloc_acc[i*2+1]);
						TIMER_STOP_(DenseScalMulSub__one_row__array_array);
					}
				}
				else	//axpy ONE ARRAY
				{
					TIMER_START_(DenseScalMulSub__one_row__array_array);
						DenseScalMulSub__one_row__array_array(
								Av1_col1,
								Av2_col1,
								Bloc_acc[Ap1],
								Bloc_acc[i*2],
								Bloc_acc[i*2+1]);
					TIMER_STOP_(DenseScalMulSub__one_row__array_array);
				}
			}

			TIMER_START_(reduceDenseArrayModulo);
				reduceDenseArrayModulo(R, Bloc_acc[i*2]);
			TIMER_STOP_(reduceDenseArrayModulo);

			if (bloc_A[i].size() > 1) //reduce lines within the same multiline
			{
				int j=bloc_A[i].size()-2;
				Ap1 = bloc_A[i].IndexData[j];
				R.copy(Av1_col1, bloc_A[i].at_unchecked(1, j));

				if (Av1_col1 != 0)
				{
					R.negin(Av1_col1);

					//TODO: make this one loop
					for (uint32 t = 0; t < DEFAULT_BLOC_WIDTH; ++t)
						Bloc_acc[i*2+1][t] += (uint32) Av1_col1 * Bloc_acc[i*2][t];
				}

			}
			TIMER_START_(reduceDenseArrayModulo);
				reduceDenseArrayModulo(R, Bloc_acc[i*2+1]);
			TIMER_STOP_(reduceDenseArrayModulo);
//			else	//these lines should be the remaning % bloc height! corresponding Bloc_acc should be null too
					//no need to reduce them in the ring
//				cout << "PROBLEM " << endl;
		}
		else	//dense
		{
			cout << "EROOR: ROWS IN A SHALL NOT BE DENSE" << endl;
			throw std::logic_error ("EROOR: ROWS IN A SHALL NOT BE DENSE");
		}

	}  //for i
}

template<typename Index>
void MatrixOps::reducePivotsByPivots(const Modular<uint16>& R,
			const SparseBlocMatrix<SparseMultilineBloc<uint16, Index> >& A,
			SparseBlocMatrix<SparseMultilineBloc<uint16, Index> >& B)
{
	std::ostream &report = commentator.report(Commentator::LEVEL_NORMAL,
			INTERNAL_DESCRIPTION);
	report << "In spec Modular<uint16> Bloc version" << std::endl;

	typedef SparseBlocMatrix<SparseMultilineBloc<uint16, Index> > Matrix;

	check_equal_or_raise_exception(A.blocArrangement, Matrix::ArrangementDownTop_RightLeft);
	check_equal_or_raise_exception(B.blocArrangement, Matrix::ArrangementDownTop_LeftRight);
	check_equal_or_raise_exception(A.rowdim(), B.rowdim());
	check_equal_or_raise_exception(A.rowdim(), A.coldim());

	check_equal_or_raise_exception(A.bloc_height(), B.bloc_height());
	check_equal_or_raise_exception(A.bloc_height(), A.bloc_width());

	uint64 *dense_bloc[DEFAULT_BLOC_HEIGHT]  __attribute__((aligned(0x1000)));
	for (uint32 i = 0; i < DEFAULT_BLOC_HEIGHT; ++i)
		//dense_bloc[i] = new uint64[B.bloc_width()];
		dense_bloc[i] = (uint64 *) memalign(64, B.bloc_width() * sizeof(uint64));

	TIMER_DECLARE_(memsetBlocToZero);
	TIMER_DECLARE_(copySparseBlocToDenseBlocArray);
	TIMER_DECLARE_(reduceBlocByRectangularBloc);
	TIMER_DECLARE_(reduceBlocByTriangularBloc);
	TIMER_DECLARE_(copyDenseBlocArrayToSparseBloc);

	TIMER_RESET_(reduceDenseArrayModulo);
	TIMER_RESET_(DenseScalMulSub__one_row__array_array);
	TIMER_RESET_(DenseScalMulSub__two_rows__array_array);
	TIMER_RESET_(SparseScalMulSub__one_row__vect_array);
	TIMER_RESET_(DenseScalMulSub__one_row__vect_array);
	TIMER_RESET_(DenseScalMulSub__two_rows__vect_array);
	TIMER_RESET_(SparseScalMulSub__two_rows__vect_array);

	TIMER_RESET_(reduceBlocByRectangularBlocOuter);
	TIMER_RESET_(reduceBlocByRectangularBlocInner);


	//for all columns of blocs of B
	for (uint32 i = 0; i < (uint32) std::ceil((double) B.coldim() / B.bloc_width()); ++i)
	{
		//report << "Column B " << i << endl;
		//for all rows of blocs in A (starting from the end)
		for (uint32 j = 0;
				j < (uint32) std::ceil((double) A.rowdim() / A.bloc_height());
				++j)
		{
			uint32 first_bloc_idx, last_bloc_idx;
			first_bloc_idx = A.FirstBlocsColumIndexes[j] / A.bloc_width();
			last_bloc_idx = min(A[j].size () - 1, j);

			//report << "\tRow A " << j << "\tfirst bloc " << first_bloc_idx << " last " << last_bloc_idx << endl;
			//1. RazBloc
			//2. copy sparse bloc to Bloc_i_j
			TIMER_START_(memsetBlocToZero);
				memsetBlocToZero(dense_bloc, B.bloc_height(), B.bloc_width());
			TIMER_STOP_(memsetBlocToZero);

			TIMER_START_(copySparseBlocToDenseBlocArray);
				copySparseBlocToDenseBlocArray(R, B[j][i], dense_bloc);
			TIMER_STOP_(copySparseBlocToDenseBlocArray);

#ifdef SHOW_PROGRESS
		report << "                                                                                    \r";
		report << "\tcolumn\t" << i << "/" << (uint32) std::ceil((double) B.coldim() / B.bloc_width()) << "\trow\t" << j << std::ends;
#endif
			//for all the blocs in the current row of A
			for (int k = 0; k < last_bloc_idx; ++k)
			{
				//report << "\t\tBloc in A and B " << k + first_bloc_idx << endl;
				TIMER_START_(reduceBlocByRectangularBloc);
					reduceBlocByRectangularBloc(R, A[j][k], B[k + first_bloc_idx][i], dense_bloc);
				TIMER_STOP_(reduceBlocByRectangularBloc);
			}

			//report << "reduceBlocByRectangularBloc() DONE" << endl;

			TIMER_START_(reduceBlocByTriangularBloc);
				reduceBlocByTriangularBloc(R, A[j][last_bloc_idx], dense_bloc);
			TIMER_STOP_(reduceBlocByTriangularBloc);
			//report << "reduceBlocByTriangularBloc DONE" << endl;

			TIMER_START_(copyDenseBlocArrayToSparseBloc);
				copyDenseBlocArrayToSparseBloc(R, dense_bloc, B[j][i], false);
			TIMER_STOP_(copyDenseBlocArrayToSparseBloc);
			//report << "copyDenseBlocArrayToSparseBloc DONE" << endl;

		}
	}
#ifdef SHOW_PROGRESS
	report << "\r                                                                                    \n";
#endif

	TIMER_REPORT_(memsetBlocToZero);
	TIMER_REPORT_(copySparseBlocToDenseBlocArray);
	TIMER_REPORT_(copyDenseBlocArrayToSparseBloc);

	report << endl;
	TIMER_REPORT_(reduceBlocByRectangularBloc);
	TIMER_REPORT_(reduceBlocByRectangularBlocOuter);
	TIMER_REPORT_(reduceBlocByRectangularBlocInner);
	TIMER_REPORT_(SparseScalMulSub__one_row__vect_array);
	TIMER_REPORT_(DenseScalMulSub__one_row__vect_array);
	TIMER_REPORT_(DenseScalMulSub__two_rows__vect_array);
	TIMER_REPORT_(SparseScalMulSub__two_rows__vect_array);

	report << endl;
	TIMER_REPORT_(reduceBlocByTriangularBloc);
	TIMER_REPORT_(DenseScalMulSub__one_row__array_array);
	TIMER_REPORT_(DenseScalMulSub__two_rows__array_array);
	TIMER_REPORT_(reduceDenseArrayModulo);


	for (uint32 i = 0; i < DEFAULT_BLOC_HEIGHT; ++i)
		free(dense_bloc[i]);
}

template<typename Index>
void MatrixOps::reduceNonPivotsByPivots(const Modular<uint16>& R,
		const SparseBlocMatrix<SparseMultilineBloc<uint16, Index> >& C,
		const SparseBlocMatrix<SparseMultilineBloc<uint16, Index> >& B,
		SparseBlocMatrix<SparseMultilineBloc<uint16, Index> >& D)
{
	std::ostream &report = commentator.report (Commentator::LEVEL_NORMAL, INTERNAL_DESCRIPTION);
	report << "In spec Modular<uint16> Bloc version" << std::endl;

	typedef SparseBlocMatrix<SparseMultilineBloc<uint16, Index> > Matrix;

	check_equal_or_raise_exception(C.blocArrangement, Matrix::ArrangementDownTop_RightLeft);
	check_equal_or_raise_exception(B.blocArrangement, Matrix::ArrangementDownTop_LeftRight);
	check_equal_or_raise_exception(D.blocArrangement, Matrix::ArrangementDownTop_LeftRight);
	check_equal_or_raise_exception(C.rowdim(), D.rowdim());
	check_equal_or_raise_exception(C.coldim(), B.rowdim());
	check_equal_or_raise_exception(B.coldim(), D.coldim());



	uint64 *dense_bloc[DEFAULT_BLOC_HEIGHT]  __attribute__((aligned(0x1000)));
	for (uint32 i = 0; i < DEFAULT_BLOC_HEIGHT; ++i)
		dense_bloc[i] = (uint64 *) memalign(64, D.bloc_width() * sizeof(uint64));

	TIMER_DECLARE_(memsetBlocToZero);
	TIMER_DECLARE_(copySparseBlocToDenseBlocArray);
	TIMER_DECLARE_(reduceBlocByRectangularBloc);
	TIMER_DECLARE_(copyDenseBlocArrayToSparseBloc);

	TIMER_RESET_(reduceDenseArrayModulo);
	TIMER_RESET_(reduceBlocByRectangularBlocOuter);
	TIMER_RESET_(reduceBlocByRectangularBlocInner);
	TIMER_RESET_(DenseScalMulSub__one_row__array_array);
	TIMER_RESET_(DenseScalMulSub__two_rows__array_array);
	TIMER_RESET_(SparseScalMulSub__one_row__vect_array);
	TIMER_RESET_(DenseScalMulSub__one_row__vect_array);
	TIMER_RESET_(DenseScalMulSub__two_rows__vect_array);
	TIMER_RESET_(SparseScalMulSub__two_rows__vect_array);

	//for all columns of blocs of B
	for (uint32 i = 0; i < (uint32) std::ceil((double) D.coldim() / D.bloc_width()); ++i)
	{
		//report << "Colum D|B\t" << i << endl;
		//for all rows of blocs in C
		for(uint32 j=0; j<(uint32)std::ceil((double)C.rowdim() / C.bloc_height()); ++j)
		{
			uint32 first_bloc_idx, last_bloc_idx;
			first_bloc_idx = C.FirstBlocsColumIndexes[j] / C.bloc_width();
			last_bloc_idx = C[j].size ();

			//report << "\tRow C\t" << j << "\tfirst bloc: " << first_bloc_idx << " - last: " << last_bloc_idx << endl;
			//1. RazBloc
			//2. copy sparse bloc to Bloc_i_j

			TIMER_START_(memsetBlocToZero);
				memsetBlocToZero(dense_bloc, D.bloc_height(), D.bloc_width());
			TIMER_STOP_(memsetBlocToZero);

			TIMER_START_(copySparseBlocToDenseBlocArray);
				copySparseBlocToDenseBlocArray(R, D[j][i], dense_bloc);
			TIMER_STOP_(copySparseBlocToDenseBlocArray);

#ifdef SHOW_PROGRESS
		report << "                                                                                    \r";
		report << "\tcolumn\t" << i << "/" << (uint32) std::ceil((double) D.coldim() / D.bloc_width()) << "\trow\t" << j << std::ends;
#endif

			//for all the blocs in the current row of C (column of B)
			for (int k = 0; k < last_bloc_idx; ++k)
			{
				//report << "\t\tBloc in C and B\t" << k + first_bloc_idx << endl;
				TIMER_START_(reduceBlocByRectangularBloc);
					reduceBlocByRectangularBloc(R, C[j][k], B[k + first_bloc_idx][i], dense_bloc);
				TIMER_STOP_(reduceBlocByRectangularBloc);
			}

			TIMER_START_(copyDenseBlocArrayToSparseBloc);
				copyDenseBlocArrayToSparseBloc(R, dense_bloc, D[j][i]);
			TIMER_STOP_(copyDenseBlocArrayToSparseBloc);
		}
	}

#ifdef SHOW_PROGRESS
	report << "\r                                                                                    \n";
#endif

	TIMER_REPORT_(memsetBlocToZero);
	TIMER_REPORT_(copySparseBlocToDenseBlocArray);
	TIMER_REPORT_(copyDenseBlocArrayToSparseBloc);

	report << endl;
	TIMER_REPORT_(reduceBlocByRectangularBloc);
	TIMER_REPORT_(reduceBlocByRectangularBlocOuter);
	TIMER_REPORT_(reduceBlocByRectangularBlocInner);
	TIMER_REPORT_(SparseScalMulSub__one_row__vect_array);
	TIMER_REPORT_(DenseScalMulSub__one_row__vect_array);
	TIMER_REPORT_(DenseScalMulSub__two_rows__vect_array);
	TIMER_REPORT_(SparseScalMulSub__two_rows__vect_array);


	for (uint32 i = 0; i < DEFAULT_BLOC_HEIGHT; ++i)
		free(dense_bloc[i]);
}




#endif /* MATRIX_OPS_H_ */




